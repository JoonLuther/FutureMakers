# FutureMakers

Reflections

7/6: I hope to learn more about the different tools I will be using throughout the program

7/7: David Kong's Leadership and Storytelling workshop reminded me of listening to the Moth Radio Hour podcast because I got to hear so many unique stories from my peers. I learned more about how other people have discovered their calling, and how I have discovered mine.

7/8: I learned that supervised learning is when the machine uses data for classification and regression and makes a prediction, and that unsupervised learning is when the machine uses data to identify patterns and form clusters of samples.

7/12: Today I learned that tensors are like matricies that store data for machine learning. I also enjoyed reading the article and running the included code. I liked seeing how the different tensor opperations are performed using code.

7/13:

7/14:

7/15 - Day 10: I enjoyed playing the survival of the fittest game and observing how the AI mimicked the way I hired hypothetical candidates. The time crunch rushed my decisions which may have contributed to me selecting more "orange" candidates than "blue" candidates. As a result, the AI was biased toward the orange candidates and hired a disproportionate amount than the population. This game reflects real life hiring practices that can be biased toward certain genders and ethnicities.

7/19 - Day 14: I followed this Hackernoon tutorial https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4 to create three different neural networks that predicted housing prices. The first model had 2 hidden layers with 32 neurons each and ReLu activation functions, and an output layer with 1 neuron and a sigmoid activation function. This model utilized the sgd optimizer. When plotted, this model's validation and training accuracy increased with epochs and were relatively close together. This model's validation and training loss decreased with epochs and were also relatively close together. The second model introduced regularization. It had 4 hidden layers with 1000 neurons each (ReLu activation functions). This model utilized the adam optimizer. When plotted, this model's validation loss actually increased with epochs whereas the training loss decreased. Additionally, this model's validation accuracy decreased with epochs even though the training accuracy increased. These are both signs of overfitting. The third model introduced dropout as well as regularization. It also had 4 hidden layers with 1000 neurons each (ReLu). This model also utilized the adam optimizer. When plotted, this model's validation and training loss decreased with epochs and were very close together. This model's validation and training accuracy increased with epochs and were also very close together.

7/20:
